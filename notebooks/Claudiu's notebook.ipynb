{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Coco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Coco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Coco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Coco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Coco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment the following to download required libs\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/original.txt', sep='\\t', quotechar='~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_labels = np.array(df['Label'])\n",
    "tweets = np.array(df['Tweet text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "tweets_tokenized = [casual_tokenize(tweet) for tweet in tweets]\n",
    "tweets_lemmatized = [[lemmatizer.lemmatize(token) for token in tweet] for tweet in tweets_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Name Entity Recognition (using SpaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to apply nlp once\n",
    "# the entire background pipeline will return the objects\n",
    "# example = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "# doc = nlp(example)\n",
    "# pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entity labels and number\n",
    "# labels = [x.label_ for x in doc.ents]\n",
    "# Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy.render(nlp(example), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy.render(nlp(example), style='dep', jupyter = True, options = {'distance': 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we verbatim extract part-of-speech and lemmatize this sentence\n",
    "# [(x.orth_,x.pos_, x.lemma_) for x in [y for y in nlp(str(example)) if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affin scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_affin = pd.read_csv('lexicons/afinn.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "affin_map = dict()\n",
    "affin_scores = list()\n",
    "for index, row in df_affin.iterrows():\n",
    "    # print(f'{row[\"word\"]} {row[\"score\"]}')\n",
    "    affin_map[row[\"word\"]] = row[\"score\"]\n",
    "    affin_scores.append(row[\"score\"])\n",
    "    \n",
    "affin_min = min(affin_scores)\n",
    "affin_max = max(affin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the normalized affin score (range [-1, 1])\n",
    "def get_affin_score(word):\n",
    "    if affin_map.get(word) is not None:\n",
    "        score = affin_map[word]\n",
    "        return 2 * ((score - affin_min)/(affin_max - affin_min)) - 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Inquirer scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel('lexicons/inquirerbasic.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi_map = dict()\n",
    "gi_scores = list()\n",
    "for i in range(len(raw_data)):\n",
    "    row = raw_data.iloc[i]\n",
    "    word = row[0]\n",
    "    positive = row[2]\n",
    "    negative = row[3]\n",
    "    score = 0\n",
    "    if positive == \"Positiv\":\n",
    "        score = 1\n",
    "    elif negative == \"Negativ\":\n",
    "        score = -1\n",
    "    if word is not True and word is not False:\n",
    "        gi_map[word.lower()] = score\n",
    "        gi_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gi_score(word):\n",
    "    if gi_map.get(word) is not None:\n",
    "        return gi_map.get(word)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPQA scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpqa = pd.read_csv('lexicons/MPQA.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpqa_map = dict()\n",
    "mpqa_scores = list()\n",
    "for index, row in df_mpqa.iterrows():\n",
    "    splits = row[0].split(' ')\n",
    "    words = splits[2].split('=')\n",
    "    scores = splits[len(splits)-1].split('=')\n",
    "    score = 0\n",
    "    if scores[1] == 'positive':\n",
    "        score = 1\n",
    "    elif scores[1] == 'negative':\n",
    "        score = -1\n",
    "    mpqa_map[words[1]] = score\n",
    "    mpqa_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mpqa_score(word):\n",
    "    if mpqa_map.get(word) is not None:\n",
    "        return mpqa_map.get(word)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liuâ€™s scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liu_pos = pd.read_csv('lexicons/liu-positive-words.txt')\n",
    "df_liu_neg = pd.read_csv('lexicons/liu-negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "liu_map = dict()\n",
    "liu_scores = list()\n",
    "for index, row in df_liu_pos.iterrows():\n",
    "    liu_map[row[\"words\"]] = 1\n",
    "    liu_scores.append(1)\n",
    "for index, row in df_liu_neg.iterrows():\n",
    "    liu_map[row[\"words\"]] = -1\n",
    "    liu_scores.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liu_score(word):\n",
    "    if liu_map.get(word) is not None:\n",
    "        return liu_map.get(word)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nrc = pd.read_csv('lexicons/NRC.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_map = dict()\n",
    "nrc_scores = list()\n",
    "item = 0\n",
    "pos_score = None\n",
    "neg_score = None\n",
    "done = False\n",
    "for index, row in df_nrc.iterrows():\n",
    "    word = row[\"word\"]\n",
    "    if item == 10:\n",
    "        item = 0\n",
    "        pos_score = None\n",
    "        neg_score = None\n",
    "        done = False\n",
    "    if item == 5:\n",
    "        neg_score = row[\"score\"]\n",
    "    if item == 6:\n",
    "        pos_score = row[\"score\"]\n",
    "    if pos_score is not None and neg_score is not None and not done:\n",
    "        if pos_score != 0 and neg_score == 0:\n",
    "            nrc_map[word] = 1\n",
    "        elif pos_score == 0 and neg_score != 0:\n",
    "            nrc_map[word] = -1\n",
    "        else:\n",
    "            nrc_map[word] = 0\n",
    "        done = True\n",
    "    item += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrc_score(word):\n",
    "    if nrc_map.get(word) is not None:\n",
    "        return nrc_map.get(word)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(document, lexicon):\n",
    "    features = list()\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "    length = len(document)\n",
    "    polarity = 0\n",
    "    maximum = -1\n",
    "    minimum = 1\n",
    "    # contrast is a binary feature (i.e: zero means no contrast)\n",
    "    contrast = 0\n",
    "    for item in document:\n",
    "        if lexicon == 'affin':\n",
    "            score = get_affin_score(item)\n",
    "        elif lexicon == 'gi':\n",
    "            score = get_gi_score(item)\n",
    "        elif lexicon == 'mpqa':\n",
    "            score = get_mpqa_score(item)\n",
    "        elif lexicon == 'liu':\n",
    "            score = get_liu_score(item)\n",
    "        else:\n",
    "            score = get_nrc_score(item)\n",
    "        polarity += score\n",
    "        maximum = max(maximum, score)\n",
    "        minimum = min(minimum, score)\n",
    "        if score > 0:\n",
    "            num_pos += 1\n",
    "        elif score < 0:\n",
    "            num_neg += 1  \n",
    "    if num_pos > 0 and num_neg > 0:\n",
    "        contrast = 1\n",
    "    features.append(num_pos / length)\n",
    "    features.append(num_neg / length)\n",
    "    features.append((length - num_pos - num_neg) / length)\n",
    "    features.append(polarity)\n",
    "    features.append(maximum - minimum)\n",
    "    features.append(contrast)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_features(corpus):\n",
    "    tokens = [casual_tokenize(item) for item in corpus]\n",
    "    features = list()\n",
    "    for item in tqdm_notebook(tokens):\n",
    "        feature = list()\n",
    "        feature += get_features(item, 'affin')\n",
    "        feature += get_features(item, 'gi')\n",
    "        feature += get_features(item, 'mpqa')\n",
    "        feature += get_features(item, 'liu')\n",
    "        feature += get_features(item, 'nrc')\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
