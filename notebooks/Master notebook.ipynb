{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature ablation + Experimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "training_df = pd.read_csv('data/train.txt', sep='\\t', quotechar='~')\n",
    "test_df = pd.read_csv('data/test.txt', sep='\\t', quotechar='~')\n",
    "\n",
    "training_true_labels = np.array(training_df['Label'])\n",
    "training_tweets = np.array(training_df['Tweet text'])\n",
    "\n",
    "test_true_labels = np.array(test_df['Label'])\n",
    "test_tweets = np.array(test_df['Tweet text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load features for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pinda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pinda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Pinda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Pinda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Pinda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "c:\\users\\pinda\\documents\\master-comp-sci-dst\\courses\\information-retrieval\\venv\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator KMeans from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pinda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb20e5c80ec44d99c3202332477aafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3834.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac08682be154767a332dd09678d69f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3834.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e83e57935a04271a618af4236a252d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3834.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e41cd6b173b43578887170f63a02287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=784.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26510d933bd04c0191753cf98a1b2c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=784.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b459c3a194b4fd88630dfff58f66a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=784.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sentiment_features\n",
    "import semantical_features\n",
    "import lexical_features\n",
    "import syntactic_features\n",
    "\n",
    "prep_training_tweets = lexical_features.preprocess(training_tweets)\n",
    "prep_test_tweets = lexical_features.preprocess(test_tweets)\n",
    "freq_list = lexical_features.get_frequencies(prep_training_tweets, 200)\n",
    "\n",
    "train_lex_features = lexical_features.get_lexical_features(prep_training_tweets, freq_list)\n",
    "train_syn_features = syntactic_features.get_syntactic_features(training_tweets)\n",
    "train_sen_features = sentiment_features.get_sentiment_features(training_tweets)\n",
    "train_sem_features = semantical_features.get_semantic_features(training_tweets)\n",
    "\n",
    "test_lex_features = lexical_features.get_lexical_features(prep_test_tweets, freq_list)\n",
    "test_syn_features = syntactic_features.get_syntactic_features(test_tweets)\n",
    "test_sen_features = sentiment_features.get_sentiment_features(test_tweets)\n",
    "test_sem_features = semantical_features.get_semantic_features(test_tweets)\n",
    "\n",
    "train_comb_features = np.hstack((train_lex_features, train_syn_features, train_sen_features, train_sem_features))\n",
    "test_comb_features = np.hstack((test_lex_features, test_syn_features, test_sen_features, test_sem_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building feature combinations (change this if you only want to run a subset of the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_feature_dict = {'Syntactic': train_syn_features, 'Semantic': train_sem_features, 'Sentiment': train_sen_features, 'Lexical': train_lex_features, 'Combined': train_comb_features}\n",
    "test_feature_dict = {'Syntactic': test_syn_features, 'Semantic': test_sem_features, 'Sentiment': test_sen_features, 'Lexical': test_lex_features, 'Combined': test_comb_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate parameter grid for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "power = -15\n",
    "gamma_values = list()\n",
    "while power <= 3:\n",
    "    gamma = 2**power\n",
    "    gamma_values.append(gamma)\n",
    "    power += 2 \n",
    " \n",
    "power = -5\n",
    "c_values = list()\n",
    "while power <= 15:\n",
    "    c = 2**power\n",
    "    c_values.append(c)\n",
    "    power += 2\n",
    "    \n",
    "param_grid = {'C': c_values, 'gamma': gamma_values,'kernel': ['rbf']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "\n",
    "scoring = {'Accuracy': 'accuracy', 'F1-score': 'f1', 'Recall': 'recall', 'Precision': 'precision'}\n",
    "\n",
    "for key in training_feature_dict.keys():\n",
    "    \n",
    "    training_features = training_feature_dict[key]\n",
    "    test_features = test_feature_dict[key]\n",
    "\n",
    "    grid = GridSearchCV(SVC(), param_grid, scoring=scoring, refit='Accuracy', verbose=10)\n",
    "    grid.fit(training_features, training_true_labels)\n",
    "    dump(grid, key + '.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering of results + evaluation using the pickled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model     |   Feature vector size |   Optimal C |   Optimal gamma |   Training avg Acc |   Training avg F1 |   Training avg Prec |   Training avg Recall |   Training std Acc |   Training std F1 |   Training std Prec |   Training std Recall |   Test Acc |   Test F1 |   Test Prec |   Test Recall |\n",
      "|-----------+-----------------------+-------------+-----------------+--------------------+-------------------+---------------------+-----------------------+--------------------+-------------------+---------------------+-----------------------+------------+-----------+-------------+---------------|\n",
      "| Lexical   |                   808 |       512   |     3.05176e-05 |           0.64397  |          0.649707 |            0.662464 |              0.637703 |         0.0149951  |        0.0142495  |           0.0181897 |            0.0165071  |   0.65051  |  0.592262 |    0.551247 |      0.639871 |\n",
      "| Syntactic |                    76 |         2   |     0.0078125   |           0.597283 |          0.619857 |            0.660391 |              0.584996 |         0.0193133  |        0.0253493  |           0.0440643 |            0.0155385  |   0.577806 |  0.552097 |    0.476636 |      0.655949 |\n",
      "| Sentiment |                    30 |         0.5 |     0.5         |           0.595201 |          0.600738 |            0.611208 |              0.590993 |         0.00694364 |        0.0079685  |           0.0174945 |            0.00829526 |   0.584184 |  0.530259 |    0.480418 |      0.59164  |\n",
      "| Semantic  |                   200 |         0.5 |     0.125       |           0.625976 |          0.622195 |            0.618505 |              0.626848 |         0.0188435  |        0.0207429  |           0.0310275 |            0.0206364  |   0.660714 |  0.602985 |    0.562674 |      0.649518 |\n",
      "| Combined  |                  1114 |      2048   |     3.05176e-05 |           0.652847 |          0.649426 |            0.645194 |              0.654799 |         0.00990869 |        0.00436742 |           0.0202563 |            0.0181025  |   0.669643 |  0.603369 |    0.576023 |      0.633441 |\n",
      "16\n",
      "Lexical & 808 & 512 & 3.0517578125e-05 & 0.6439697577282212 & 0.6497070124226582 & 0.6624636036799585 & 0.6377026216204179 & 0.01499509612754582 & 0.014249485009665908 & 0.01818971475028395 & 0.016507084829488435 & 0.6505102040816326 & 0.5922619047619048 & 0.5512465373961218 & 0.639871382636656\n",
      "16\n",
      "Syntactic & 76 & 2 & 0.0078125 & 0.597283165566566 & 0.6198571243910576 & 0.6603912348092356 & 0.5849959315759816 & 0.019313265735508036 & 0.025349345652176305 & 0.0440642719759304 & 0.015538455657915923 & 0.5778061224489796 & 0.5520974289580514 & 0.4766355140186916 & 0.6559485530546624\n",
      "16\n",
      "Sentiment & 30 & 0.5 & 0.5 & 0.5952005201507349 & 0.600738107069635 & 0.6112080160758958 & 0.5909931495372684 & 0.006943636275753257 & 0.007968504567461334 & 0.017494454088972485 & 0.008295257498809012 & 0.5841836734693877 & 0.5302593659942363 & 0.4804177545691906 & 0.5916398713826366\n",
      "16\n",
      "Semantic & 200 & 0.5 & 0.125 & 0.6259755379373029 & 0.6221949271150371 & 0.618505051057373 & 0.6268481874298594 & 0.01884353453258218 & 0.02074293458390824 & 0.031027454130667882 & 0.0206363957095152 & 0.6607142857142857 & 0.6029850746268656 & 0.5626740947075209 & 0.6495176848874598\n",
      "16\n",
      "Combined & 1114 & 2048 & 3.0517578125e-05 & 0.6528470423235214 & 0.6494259997828105 & 0.6451943187565787 & 0.6547990386456005 & 0.00990868560597 & 0.004367419222629114 & 0.02025628196456283 & 0.01810248226478575 & 0.6696428571428571 & 0.6033690658499234 & 0.5760233918128655 & 0.6334405144694534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tabulate import tabulate\n",
    "from joblib import dump, load\n",
    "\n",
    "keyset = ['Lexical', 'Syntactic', 'Sentiment', 'Semantic', 'Combined']\n",
    "metrics = ['Accuracy', 'F1-score', 'Recall', 'Precision']\n",
    "results = []\n",
    "\n",
    "\n",
    "for key in keyset:\n",
    "    model = load(key + '.joblib') \n",
    "    test_features = test_feature_dict[key]\n",
    "    feature_vec_size = len(test_features[0])\n",
    "    \n",
    "    optimal_params = model.cv_results_['params'][model.best_index_]\n",
    "    \n",
    "    avg_metrics = []\n",
    "    std_metrics = []\n",
    "    for metric in metrics:\n",
    "        avg_metric = model.cv_results_['mean_test_' + metric][model.best_index_]\n",
    "        std_metric = model.cv_results_['std_test_' + metric][model.best_index_]\n",
    "        avg_metrics.append(avg_metric)\n",
    "        std_metrics.append(std_metric)\n",
    "    \n",
    "    predictions = model.best_estimator_.predict(test_features)\n",
    "    test_accuracy = accuracy_score(test_true_labels, predictions)\n",
    "    test_f1 = f1_score(test_true_labels, predictions)\n",
    "    test_precision = precision_score(test_true_labels, predictions)\n",
    "    test_recall = recall_score(test_true_labels, predictions)\n",
    "    \n",
    "    results.append([key, feature_vec_size, optimal_params['C'], optimal_params['gamma']] + avg_metrics + std_metrics + [test_accuracy, test_f1, test_precision, test_recall])\n",
    "\n",
    "print(tabulate(results, headers=['Model', 'Feature vector size', 'Optimal C', 'Optimal gamma', 'Training avg Acc', 'Training avg F1', 'Training avg Prec', 'Training avg Recall', 'Training std Acc', 'Training std F1', 'Training std Prec', 'Training std Recall', 'Test Acc', 'Test F1', 'Test Prec', 'Test Recall'], tablefmt='orgtbl'))\n",
    "\n",
    "for result in results:\n",
    "    print(' & '.join(map(str,result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
